{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1854a92c-951f-4837-aaaf-839ab609ea2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline,PegasusTokenizer,PegasusForConditionalGeneration,T5Tokenizer,T5ForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e70daa38-8d7d-43f5-a168-d19d4a060b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "concatinated_texts = \"\"\"Today is our dragon boat festival, in our Chinese culture, to celebrate it with all safe and great in\n",
    "our lives. Hope you too, to enjoy it as my deepest wishes.\n",
    "Thank your message to show our words to the doctor, as his next contract checking, to all of us.\n",
    "I got this message to see the approved message. In fact, I have received the message from the\n",
    "professor, to show me, this, a couple of days ago. I am very appreciated the full support of the\n",
    "professor, for our Springer proceedings publication\n",
    "“During our final discuss, I told him about the new submission — the one we were waiting since\n",
    "last autumn, but the updates was confusing as it not included the full feedback from reviewer or\n",
    "maybe editor?Anyway, I believe the team, although bit delay and less communication at recent days, they really\n",
    "tried best for paper and cooperation. We should be grateful, I mean all of us, for the acceptance\n",
    "and efforts until the Springer link came finally last week, I think.\n",
    "Also, kindly remind me please, if the doctor still plan for the acknowledgments section edit before\n",
    "he sending again. Because I didn’t see that part final yet, or maybe I missed, I apologize if so.\n",
    "Overall, let us make sure all are safe and celebrate the outcome with strong coffee and future\n",
    "targets”\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "314b4940-6f8d-48a3-b2d1-30d65f2aa184",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Περίληψη: The author received the approved version of the manuscript a few days ago. The manuscript was submitted to Springer for publication last week. The author says he is grateful for the full support of the Springer team.\n"
     ]
    }
   ],
   "source": [
    "# pipeline-bart-large για περίληψη,χρησιμοποιωντας cpu\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "summary = summarizer(concatinated_texts, max_length=200, min_length=20, do_sample=False)\n",
    "\n",
    "print(\"Περίληψη:\", summary[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d89e52b6-8e10-4390-a5f0-5b0be596f78d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-large and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thank your message to show our words to the doctor, as his next contract checking, to all of us. I am very appreciated the full support of the professor, for our Springer proceedings publication “During our final discuss, I told him about the new submission — the one we were waiting since last autumn, but the updates was confusing as it not included the complete feedback from reviewer or maybe editor?Anyway,I believe the team, although bit delay and less communication at recent days, they really\n"
     ]
    }
   ],
   "source": [
    "model_name = \"google/pegasus-large\"\n",
    "\n",
    "# Φορτώνουμε explicit τον tokenizer και το μοντέλο\n",
    "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "model = PegasusForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Tokenize το input\n",
    "inputs = tokenizer(concatinated_texts, return_tensors=\"pt\", truncation=True)\n",
    "\n",
    "# Παράγουμε την ανακατασκευή / σύνοψη\n",
    "summary_ids = model.generate(\n",
    "    **inputs,\n",
    "    max_length=100,\n",
    "    min_length=30,\n",
    "    no_repeat_ngram_size=2,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "# Μετατρέπουμε τα tokens πίσω σε κείμενο\n",
    "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5b0b386-cca1-4092-ba14-a63a48bd8702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Falkenstein: today is our dragon boat festival, in our Chinese culture, to celebrate it with all safe and great . I am very grateful, all of us, for the acceptance and efforts until the Springer link came finally last week, I think.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"t5-base\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "\n",
    "# Προετοιμασία του input για T5 (prefix \"paraphrase: \" για παραφράσεις)\n",
    "input_text = \"paraphrase: \" + concatinated_texts\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "\n",
    "# Δημιουργία ανακατασκευής / παραφράσεων\n",
    "output = model.generate(\n",
    "    **inputs,\n",
    "    max_length=250,\n",
    "    min_length=30,\n",
    "    num_beams=5,\n",
    "    no_repeat_ngram_size=2,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "# Αποκωδικοποίηση παραφρασμένων κειμένων\n",
    "final_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(final_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee26f874-bce1-420c-8fe5-bb3c48893ab4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
